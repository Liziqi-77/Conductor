# vLLM 服务层（Serving Layer）架构详细分析

## 1. 服务层概述

服务层是vLLM对外提供API服务的核心组件，主要负责：
- 接收和处理HTTP请求
- 将请求转换为引擎可理解的格式
- 调用引擎进行推理
- 将引擎输出格式化为OpenAI兼容的响应
- 支持流式和非流式响应

## 2. 服务层架构组件

### 2.1 核心文件结构

```
vllm/entrypoints/openai/
├── api_server.py          # FastAPI应用和路由定义
├── serving_engine.py      # 基础服务类（OpenAIServing）
├── serving_chat.py        # Chat Completions API实现
├── serving_completion.py  # Completions API实现
├── serving_responses.py   # Responses API实现
├── serving_tokenization.py # Tokenization API实现
├── serving_models.py      # 模型管理
├── serving_tokens.py      # Tokens API实现
└── protocol.py            # 请求/响应协议定义
```

## 3. API服务器（api_server.py）

### 3.1 核心函数

#### 3.1.1 `build_app(args: Namespace) -> FastAPI`
**功能**：构建FastAPI应用实例

**关键步骤**：
1. 创建FastAPI应用（支持/禁用文档）
2. 注册路由（Chat、Completion、Embedding等）
3. 添加中间件（CORS、认证、SSL等）
4. 配置异常处理器
5. 挂载Prometheus指标

**代码位置**：`vllm/entrypoints/openai/api_server.py:1349`

**关键代码**：
```python
def build_app(args: Namespace) -> FastAPI:
    app = FastAPI(lifespan=lifespan)
    
    # 注册路由
    app.include_router(router)
    
    # 添加中间件
    app.add_middleware(CORSMiddleware, ...)
    app.add_middleware(AuthenticationMiddleware, ...)
    
    # 异常处理
    @app.exception_handler(HTTPException)
    async def http_exception_handler(...):
        ...
    
    return app
```

#### 3.1.2 `run_server_worker(...)`
**功能**：运行单个API服务器工作进程

**执行流程**：
1. 构建异步引擎客户端（`build_async_engine_client`）
2. 构建FastAPI应用（`build_app`）
3. 初始化应用状态（`init_app_state`）
4. 启动HTTP服务器（`serve_http`）

**代码位置**：`vllm/entrypoints/openai/api_server.py:1822`

**关键代码**：
```python
async def run_server_worker(...):
    async with build_async_engine_client(args) as engine_client:
        app = build_app(args)
        await init_app_state(engine_client, app.state, args)
        
        shutdown_task = await serve_http(app, ...)
        await shutdown_task
```

#### 3.1.3 `init_app_state(...)`
**功能**：初始化应用状态，设置各种服务处理器

**关键步骤**：
1. 获取支持的模型和任务
2. 处理chat template
3. 初始化工具服务器（如果启用）
4. 创建各种服务处理器：
   - `OpenAIServingChat` - Chat Completions
   - `OpenAIServingCompletion` - Completions
   - `OpenAIServingEmbedding` - Embeddings
   - `OpenAIServingResponses` - Responses
   - 等等

**代码位置**：`vllm/entrypoints/openai/api_server.py:1494`

#### 3.1.4 `build_async_engine_client(...)`
**功能**：构建异步引擎客户端

**执行流程**：
1. 从命令行参数创建`AsyncEngineArgs`
2. 创建`VllmConfig`
3. 创建`AsyncLLM`实例
4. 返回引擎客户端（作为上下文管理器）

**代码位置**：`vllm/entrypoints/openai/api_server.py:157`

### 3.2 路由定义

#### 3.2.1 Chat Completions路由
```python
@router.post("/v1/chat/completions")
async def create_chat_completion(
    request: ChatCompletionRequest,
    raw_request: Request
):
    handler = chat(raw_request)
    if handler is None:
        return error_response("Chat not supported")
    
    result = await handler.create_chat_completion(request, raw_request)
    
    if request.stream:
        return StreamingResponse(result, media_type="text/event-stream")
    return JSONResponse(content=result.model_dump())
```

#### 3.2.2 Completions路由
```python
@router.post("/v1/completions")
async def create_completion(
    request: CompletionRequest,
    raw_request: Request
):
    handler = completion(raw_request)
    result = await handler.create_completion(request, raw_request)
    # 处理流式/非流式响应
```

## 4. 基础服务类（serving_engine.py）

### 4.1 `OpenAIServing`类

这是所有服务类的基类，提供了通用的功能。

#### 4.1.1 `__init__()`
**功能**：初始化基础服务类

**关键属性**：
- `engine_client`: 引擎客户端
- `models`: 模型管理器
- `request_logger`: 请求日志记录器
- `input_processor`: 输入处理器
- `io_processor`: IO处理器
- `model_config`: 模型配置

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:256`

#### 4.1.2 `_preprocess_chat(...)`
**功能**：预处理Chat格式的请求

**执行步骤**：
1. 解析消息（`parse_chat_messages_futures`）
2. 应用chat template（`apply_hf_chat_template`或`apply_mistral_chat_template`）
3. 处理多模态数据
4. Tokenize提示文本
5. 构建引擎提示（`EngineTokensPrompt`）

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1073`

**关键代码**：
```python
async def _preprocess_chat(...):
    # 1. 解析消息
    conversation, mm_data_future, mm_uuids = parse_chat_messages_futures(
        messages, model_config, tokenizer, ...
    )
    
    # 2. 应用chat template
    if isinstance(tokenizer, MistralTokenizer):
        request_prompt = await self._apply_mistral_chat_template_async(...)
    else:
        request_prompt = apply_hf_chat_template(...)
    
    # 3. Tokenize
    prompt_inputs = await self._tokenize_prompt_input_async(...)
    
    # 4. 构建引擎提示
    engine_prompt = EngineTokensPrompt(
        prompt_token_ids=prompt_inputs["prompt_token_ids"]
    )
    
    return conversation, [request_prompt], [engine_prompt]
```

#### 4.1.3 `_process_inputs(...)`
**功能**：处理输入，转换为`EngineCoreRequest`

**执行步骤**：
1. 验证截断大小
2. 使用`input_processor.process_inputs()`处理
3. 返回`EngineCoreRequest`和tokenization参数

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1200`

#### 4.1.4 `_tokenize_prompt_input_async(...)`
**功能**：异步tokenize提示输入

**支持的类型**：
- 字符串：直接tokenize
- Token IDs列表：验证和截断
- 嵌入张量：直接使用

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1009`

#### 4.1.5 `_check_model(...)`
**功能**：检查模型是否存在且可用

**检查逻辑**：
1. 检查是否是基础模型
2. 检查是否是LoRA适配器
3. 如果启用运行时LoRA更新，尝试解析LoRA

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:772`

#### 4.1.6 `_maybe_get_adapters(...)`
**功能**：获取LoRA适配器请求

**逻辑**：
1. 如果请求的model是LoRA，返回对应的`LoRARequest`
2. 如果支持默认多模态LoRA，尝试匹配
3. 如果是基础模型，返回None

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:824`

#### 4.1.7 `create_error_response(...)`
**功能**：创建错误响应

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:743`

## 5. Chat Completions服务（serving_chat.py）

### 5.1 `OpenAIServingChat`类

继承自`OpenAIServing`，专门处理Chat Completions API。

#### 5.1.1 `create_chat_completion(...)`
**功能**：处理Chat Completions请求的主函数

**执行流程**：
1. **模型检查**：`_check_model(request)`
2. **引擎状态检查**：检查引擎是否已死亡
3. **获取LoRA适配器**：`_maybe_get_adapters(...)`
4. **获取Tokenizer**：`await engine_client.get_tokenizer()`
5. **工具处理**：处理工具调用相关逻辑
6. **预处理Chat**：`_preprocess_chat(...)`
   - 解析消息
   - 应用chat template
   - Tokenize
7. **创建采样参数**：`request.to_sampling_params(...)`
8. **处理输入**：`_process_inputs(...)`
9. **调用引擎生成**：`engine_client.generate(...)`
10. **处理响应**：
    - 流式：`chat_completion_stream_generator(...)`
    - 非流式：`chat_completion_full_generator(...)`

**代码位置**：`vllm/entrypoints/openai/serving_chat.py:161`

**关键代码流程**：
```python
async def create_chat_completion(...):
    # 1. 检查模型
    error_check_ret = await self._check_model(request)
    if error_check_ret is not None:
        return error_check_ret
    
    # 2. 获取LoRA
    lora_request = self._maybe_get_adapters(...)
    
    # 3. 获取tokenizer
    tokenizer = await self.engine_client.get_tokenizer()
    
    # 4. 预处理
    conversation, request_prompts, engine_prompts = await self._preprocess_chat(...)
    
    # 5. 创建采样参数
    sampling_params = request.to_sampling_params(...)
    
    # 6. 处理输入
    engine_request, tokenization_kwargs = await self._process_inputs(...)
    
    # 7. 调用引擎
    generator = self.engine_client.generate(
        engine_request,
        sampling_params,
        request_id,
        ...
    )
    
    # 8. 处理响应
    if request.stream:
        return self.chat_completion_stream_generator(...)
    else:
        return await self.chat_completion_full_generator(...)
```

#### 5.1.2 `chat_completion_stream_generator(...)`
**功能**：生成流式Chat Completions响应

**执行流程**：
1. 遍历`result_generator`（引擎输出）
2. 对每个输出：
   - 提取生成的文本
   - 处理工具调用（如果有）
   - 处理推理内容（如果有）
   - 构建`ChatCompletionStreamResponse`
   - 序列化为JSON字符串
   - 按照SSE格式发送（`data: {...}\n\n`）
3. 发送最终块（`[DONE]`）

**代码位置**：`vllm/entrypoints/openai/serving_chat.py:527`

**关键代码**：
```python
async def chat_completion_stream_generator(...):
    created_time = int(time.time())
    first_iteration = True
    
    async for res in result_generator:
        # 提取文本
        delta_text = self._get_delta_text(res, previous_text)
        
        # 处理工具调用
        if tool_parser:
            delta_message, function_name_returned = self.extract_tool_call_required_streaming(...)
        
        # 构建响应
        choice = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(content=delta_text, ...),
            finish_reason=None,
        )
        
        chunk = ChatCompletionStreamResponse(
            id=request_id,
            choices=[choice],
            created=created_time,
            model=model_name,
        )
        
        # 发送SSE格式
        yield f"data: {chunk.model_dump_json()}\n\n"
    
    # 发送完成标记
    yield "data: [DONE]\n\n"
```

#### 5.1.3 `chat_completion_full_generator(...)`
**功能**：生成非流式Chat Completions响应

**执行流程**：
1. 收集所有引擎输出
2. 提取最终文本
3. 处理工具调用
4. 构建`ChatCompletionResponse`
5. 返回完整响应

**代码位置**：`vllm/entrypoints/openai/serving_chat.py:700+`

#### 5.1.4 `extract_tool_call_required_streaming(...)`
**功能**：从流式输出中提取工具调用

**逻辑**：
1. 使用`partial_json_parser`解析部分JSON
2. 检查工具调用是否完整
3. 提取函数名和参数
4. 构建`DeltaMessage`包含工具调用信息

**代码位置**：`vllm/entrypoints/openai/serving_chat.py:431`

#### 5.1.5 `_preprocess_chat(...)`
**功能**：预处理Chat请求（重写父类方法）

**特殊处理**：
- 支持Harmony格式（GPT-OSS）
- 支持工具解析
- 支持多模态数据

**代码位置**：`vllm/entrypoints/openai/serving_chat.py:237`

## 6. Completions服务（serving_completion.py）

### 6.1 `OpenAIServingCompletion`类

继承自`OpenAIServing`，处理Completions API。

#### 6.1.1 `create_completion(...)`
**功能**：处理Completions请求

**执行流程**：
1. 检查模型
2. 获取LoRA适配器
3. 使用Renderer渲染提示：
   - 支持文本提示
   - 支持提示嵌入（`prompt_embeds`）
4. 创建采样参数
5. 调用引擎生成
6. 处理响应（流式/非流式）

**代码位置**：`vllm/entrypoints/openai/serving_completion.py:79`

**关键代码**：
```python
async def create_completion(...):
    # 1. 检查模型
    error_check_ret = await self._check_model(request)
    
    # 2. 获取LoRA
    lora_request = self._maybe_get_adapters(request)
    
    # 3. 渲染提示
    renderer = self._get_renderer(tokenizer)
    engine_prompts = await renderer.render_prompt_and_embeds(
        prompt_or_prompts=request.prompt,
        prompt_embeds=request.prompt_embeds,
        config=self._build_render_config(request),
    )
    
    # 4. 创建采样参数
    sampling_params = request.to_sampling_params(...)
    
    # 5. 调用引擎
    generator = self.engine_client.generate(...)
    
    # 6. 处理响应
    if request.stream:
        return self.completion_stream_generator(...)
    else:
        return await self.completion_full_generator(...)
```

## 7. 关键辅助函数

### 7.1 输入处理相关

#### 7.1.1 `_normalize_prompt_text_to_input(...)`
**功能**：将文本提示标准化为输入格式

**步骤**：
1. 如果启用小写，转换为小写
2. 根据`truncate_prompt_tokens`决定是否截断
3. Tokenize文本
4. 验证输入长度

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:870`

#### 7.1.2 `_normalize_prompt_tokens_to_input(...)`
**功能**：将Token IDs标准化为输入格式

**步骤**：
1. 根据`truncate_prompt_tokens`截断
2. 如果需要，解码为文本
3. 验证输入长度

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:912`

#### 7.1.3 `_validate_input(...)`
**功能**：验证输入长度

**验证规则**：
- Embedding/Classification/Score请求：输入长度可以到模型最大长度
- Tokenize/Detokenize请求：无长度限制
- 其他请求：输入长度 + max_tokens <= 模型最大长度

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:935`

### 7.2 响应处理相关

#### 7.2.1 `_get_decoded_token(...)`
**功能**：获取解码后的token

**逻辑**：
- 如果`return_as_token_id=True`，返回`token_id:xxx`格式
- 如果logprob中有`decoded_token`，直接使用
- 否则使用tokenizer解码

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1441`

### 7.3 工具处理相关

#### 7.3.1 `_parse_tool_calls_from_content(...)`
**功能**：从内容中解析工具调用

**支持的tool_choice模式**：
- `ToolChoiceFunction`：强制调用指定函数
- `ChatCompletionNamedToolChoiceParam`：强制调用命名工具
- `"required"`：必须调用工具
- `"auto"`：自动解析工具调用（使用tool_parser）

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1370`

### 7.4 请求ID和追踪

#### 7.4.1 `_base_request_id(...)`
**功能**：获取基础请求ID

**逻辑**：
- 如果请求头中有`X-Request-Id`，使用它
- 否则生成新的UUID

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1343`

#### 7.4.2 `_get_trace_headers(...)`
**功能**：从请求头中提取追踪头

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1328`

#### 7.4.3 `_get_data_parallel_rank(...)`
**功能**：从请求头中获取数据并行rank

**代码位置**：`vllm/entrypoints/openai/serving_engine.py:1355`

## 8. 请求处理流程总结

### 8.1 Chat Completions请求流程

```
HTTP请求 (POST /v1/chat/completions)
  ↓
FastAPI路由处理
  ↓
OpenAIServingChat.create_chat_completion()
  ├── _check_model() - 检查模型
  ├── _maybe_get_adapters() - 获取LoRA
  ├── get_tokenizer() - 获取tokenizer
  ├── _preprocess_chat() - 预处理
  │   ├── parse_chat_messages_futures() - 解析消息
  │   ├── apply_hf_chat_template() - 应用模板
  │   └── _tokenize_prompt_input_async() - Tokenize
  ├── to_sampling_params() - 创建采样参数
  ├── _process_inputs() - 处理输入
  │   └── input_processor.process_inputs() - 转换为EngineCoreRequest
  ├── engine_client.generate() - 调用引擎
  │   └── AsyncLLM.add_request() - 添加请求到引擎
  └── chat_completion_stream_generator() / chat_completion_full_generator()
      └── 格式化响应
```

### 8.2 Completions请求流程

```
HTTP请求 (POST /v1/completions)
  ↓
FastAPI路由处理
  ↓
OpenAIServingCompletion.create_completion()
  ├── _check_model()
  ├── _maybe_get_adapters()
  ├── renderer.render_prompt_and_embeds() - 渲染提示
  ├── to_sampling_params()
  ├── engine_client.generate()
  └── completion_stream_generator() / completion_full_generator()
```

## 9. 关键设计模式

### 9.1 异步生成器模式
- 使用`AsyncGenerator`实现流式响应
- 引擎返回异步生成器，服务层逐项处理并格式化

### 9.2 模板方法模式
- `OpenAIServing`定义通用流程
- 子类重写特定步骤（如`_preprocess_chat`）

### 9.3 依赖注入
- 通过FastAPI的`Depends`注入服务实例
- 如`chat(request: Request) -> OpenAIServingChat`

### 9.4 错误处理
- 统一的错误响应格式（`ErrorResponse`）
- 异常转换为HTTP状态码

## 10. 总结

服务层是vLLM的API接口层，主要特点：

1. **OpenAI兼容**：完全兼容OpenAI API规范
2. **异步处理**：全面使用异步I/O提高并发性能
3. **模块化设计**：不同API类型有独立的服务类
4. **灵活的输入处理**：支持文本、Token IDs、嵌入等多种输入格式
5. **流式和非流式支持**：同时支持两种响应模式
6. **工具调用支持**：支持函数调用和工具使用
7. **多模态支持**：支持图像、音频等多模态输入

服务层通过清晰的职责划分和良好的抽象，实现了高效、灵活的API服务。

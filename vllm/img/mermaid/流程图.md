# vLLM 架构图和执行流程图

本文档包含vLLM的整体架构图和详细的执行流程顺序图。

## 1. 整体架构图

下图展示了vLLM的分层架构和主要组件之间的关系：

```mermaid
graph TB
    subgraph "入口层 (Entry Points)"
        CLI[CLI主入口<br/>main.py]
        Serve[Serve子命令<br/>serve.py]
    end
    
    subgraph "服务层 (Serving Layer)"
        API[FastAPI服务器<br/>api_server.py]
        Chat[Chat服务处理<br/>serving_chat.py]
        Comp[Completion服务<br/>serving_completion.py]
    end
    
    subgraph "引擎层 (Engine Layer)"
        AsyncLLM[AsyncLLM<br/>async_llm.py]
        LLMEngine[LLMEngine<br/>llm_engine.py]
        InputProc[InputProcessor<br/>input_processor.py]
        OutputProc[OutputProcessor<br/>output_processor.py]
    end
    
    subgraph "核心层 (Core Layer)"
        EngineCore[EngineCore<br/>core.py]
        Scheduler[Scheduler<br/>scheduler.py]
        KVCache[KV缓存管理<br/>kv_cache_manager.py]
        RequestMgr[请求管理<br/>request.py]
    end
    
    subgraph "执行层 (Executor Layer)"
        Executor[Executor抽象<br/>abstract.py]
        UniProc[单进程执行器<br/>uniproc_executor.py]
        MultiProc[多进程执行器<br/>multiproc_executor.py]
        RayExec[Ray执行器<br/>ray_executor.py]
    end
    
    subgraph "工作层 (Worker Layer)"
        Worker[Worker<br/>gpu_worker.py]
        ModelRunner[ModelRunner<br/>gpu_model_runner.py]
        Model[LLM模型<br/>model_executor]
    end
    
    CLI --> Serve
    Serve --> API
    API --> Chat
    API --> Comp
    Chat --> AsyncLLM
    Comp --> AsyncLLM
    AsyncLLM --> InputProc
    AsyncLLM --> OutputProc
    AsyncLLM --> EngineCore
    LLMEngine --> EngineCore
    EngineCore --> Scheduler
    EngineCore --> KVCache
    EngineCore --> Executor
    Scheduler --> RequestMgr
    Scheduler --> KVCache
    Executor --> UniProc
    Executor --> MultiProc
    Executor --> RayExec
    UniProc --> Worker
    MultiProc --> Worker
    RayExec --> Worker
    Worker --> ModelRunner
    ModelRunner --> Model
    
    style CLI fill:#e1f5ff
    style Serve fill:#e1f5ff
    style API fill:#fff4e1
    style Chat fill:#fff4e1
    style Comp fill:#fff4e1
    style AsyncLLM fill:#e8f5e9
    style LLMEngine fill:#e8f5e9
    style EngineCore fill:#f3e5f5
    style Scheduler fill:#f3e5f5
    style KVCache fill:#f3e5f5
    style Executor fill:#fce4ec
    style Worker fill:#e0f2f1
    style ModelRunner fill:#e0f2f1
    style Model fill:#e0f2f1
```

## 2. 详细组件架构图

下图展示了更详细的组件内部结构和数据流：

```mermaid
graph LR
    subgraph "HTTP请求处理"
        HTTP[HTTP请求]
        FastAPI[FastAPI路由]
        Middleware[中间件<br/>CORS/认证/日志]
    end
    
    subgraph "请求预处理"
        ChatTemplate[Chat Template处理]
        Tokenizer[Tokenizer<br/>文本→Token IDs]
        InputVal[输入验证]
    end
    
    subgraph "引擎核心循环"
        AddReq[添加请求<br/>add_request]
        Schedule[调度器<br/>schedule]
        ExecModel[执行模型<br/>execute_model]
        Sample[采样Token<br/>sample_tokens]
        Update[更新状态<br/>update_from_output]
    end
    
    subgraph "KV缓存管理"
        Alloc[分配缓存块]
        Swap[交换缓存<br/>GPU↔CPU]
        Prefix[前缀缓存]
    end
    
    subgraph "模型执行"
        Prepare[准备输入]
        Forward[前向传播]
        Attention[注意力计算]
        Logits[Logits输出]
    end
    
    subgraph "输出处理"
        Format[格式化输出]
        Stream[流式输出]
        Response[HTTP响应]
    end
    
    HTTP --> FastAPI
    FastAPI --> Middleware
    Middleware --> ChatTemplate
    ChatTemplate --> Tokenizer
    Tokenizer --> InputVal
    InputVal --> AddReq
    AddReq --> Schedule
    Schedule --> Alloc
    Schedule --> ExecModel
    Alloc --> Swap
    Alloc --> Prefix
    ExecModel --> Prepare
    Prepare --> Forward
    Forward --> Attention
    Attention --> Logits
    Logits --> Sample
    Sample --> Update
    Update --> Format
    Format --> Stream
    Stream --> Response
    
    style HTTP fill:#ffebee
    style FastAPI fill:#fff4e1
    style Schedule fill:#e8f5e9
    style ExecModel fill:#e1f5ff
    style Sample fill:#f3e5f5
    style Response fill:#ffebee
```

## 3. 单请求执行流程顺序图

下图展示了单个Chat Completion请求从接收到响应的完整执行流程：

```mermaid
sequenceDiagram
    participant Client as 客户端
    participant FastAPI as FastAPI服务器
    participant ChatServing as OpenAIServingChat
    participant AsyncLLM as AsyncLLM
    participant InputProc as InputProcessor
    participant EngineCore as EngineCore
    participant Scheduler as Scheduler
    participant KVCache as KV缓存管理
    participant Executor as Executor
    participant Worker as Worker
    participant ModelRunner as ModelRunner
    participant Model as LLM模型
    participant OutputProc as OutputProcessor
    
    Client->>FastAPI: POST /v1/chat/completions
    FastAPI->>ChatServing: create_chat_completion()
    
    Note over ChatServing: 1. 验证请求和模型
    ChatServing->>ChatServing: _check_model()
    ChatServing->>ChatServing: _preprocess_chat()
    
    Note over ChatServing: 2. 应用Chat Template
    ChatServing->>ChatServing: 应用chat template
    ChatServing->>ChatServing: 转换为engine_prompt
    
    Note over ChatServing: 3. 创建采样参数
    ChatServing->>ChatServing: to_sampling_params()
    
    Note over ChatServing,InputProc: 4. 处理输入
    ChatServing->>AsyncLLM: generate()
    AsyncLLM->>InputProc: process_inputs()
    InputProc->>InputProc: Tokenization
    InputProc->>InputProc: 创建EngineCoreRequest
    InputProc-->>AsyncLLM: EngineCoreRequest
    
    Note over AsyncLLM,EngineCore: 5. 添加到引擎
    AsyncLLM->>EngineCore: add_request()
    EngineCore->>Scheduler: add_request()
    Scheduler->>Scheduler: 添加到等待队列
    
    Note over EngineCore,Scheduler: 6. 调度循环开始
    loop 每个推理步骤
        EngineCore->>Scheduler: schedule()
        
        Note over Scheduler: 6.1 选择要执行的请求
        Scheduler->>Scheduler: 从等待队列选择请求
        Scheduler->>Scheduler: 从运行队列选择请求
        
        Note over Scheduler,KVCache: 6.2 分配KV缓存
        Scheduler->>KVCache: 分配缓存块
        KVCache-->>Scheduler: 缓存块信息
        
        Note over Scheduler: 6.3 构建调度输出
        Scheduler->>Scheduler: 构建SchedulerOutput
        Scheduler-->>EngineCore: SchedulerOutput
        
        Note over EngineCore,Model: 6.4 执行模型
        EngineCore->>Executor: execute_model()
        Executor->>Worker: execute_model()
        Worker->>ModelRunner: execute_model()
        ModelRunner->>ModelRunner: 准备输入batch
        ModelRunner->>Model: forward()
        Model->>Model: 前向传播
        Model->>Model: 注意力计算
        Model-->>ModelRunner: logits
        ModelRunner-->>Worker: ModelRunnerOutput
        Worker-->>Executor: ModelRunnerOutput
        Executor-->>EngineCore: ModelRunnerOutput
        
        Note over EngineCore: 6.5 采样Token
        EngineCore->>Executor: sample_tokens()
        Executor->>Worker: sample_tokens()
        Worker->>Worker: 采样算法
        Worker-->>Executor: sampled_tokens
        Executor-->>EngineCore: sampled_tokens
        
        Note over EngineCore,Scheduler: 6.6 更新状态
        EngineCore->>Scheduler: update_from_output()
        Scheduler->>Scheduler: 更新请求状态
        Scheduler->>Scheduler: 检查完成条件
        Scheduler-->>EngineCore: EngineCoreOutputs
        
        Note over EngineCore,OutputProc: 6.7 处理输出
        EngineCore->>OutputProc: process_outputs()
        OutputProc->>OutputProc: 格式化输出
        OutputProc-->>EngineCore: RequestOutput
        EngineCore-->>AsyncLLM: RequestOutput
        AsyncLLM-->>ChatServing: RequestOutput
        
        Note over ChatServing: 6.8 流式返回
        ChatServing->>Client: 流式返回token
    end
    
    Note over ChatServing: 7. 请求完成
    ChatServing->>ChatServing: 格式化最终响应
    ChatServing->>FastAPI: ChatCompletionResponse
    FastAPI->>Client: HTTP 200 OK
```

## 4. 多请求批处理执行流程顺序图

下图展示了多个请求如何被批处理并一起执行的流程：

```mermaid
sequenceDiagram
    participant Req1 as 请求1
    participant Req2 as 请求2
    participant Req3 as 请求3
    participant API as API服务器
    participant EngineCore as EngineCore
    participant Scheduler as Scheduler
    participant Batch as 批处理
    participant Model as 模型
    
    Note over Req1,Req2,Req3: 请求到达
    Req1->>API: HTTP请求1
    Req2->>API: HTTP请求2
    Req3->>API: HTTP请求3
    
    Note over API: 并行处理请求
    par 请求1处理
        API->>EngineCore: add_request(req1)
        EngineCore->>Scheduler: add_request(req1)
        Scheduler->>Scheduler: 添加到等待队列
    and 请求2处理
        API->>EngineCore: add_request(req2)
        EngineCore->>Scheduler: add_request(req2)
        Scheduler->>Scheduler: 添加到等待队列
    and 请求3处理
        API->>EngineCore: add_request(req3)
        EngineCore->>Scheduler: add_request(req3)
        Scheduler->>Scheduler: 添加到等待队列
    end
    
    Note over Scheduler: 调度器批处理
    loop 每个推理步骤
        Scheduler->>Scheduler: schedule()
        
        Note over Scheduler: 选择可执行的请求
        Scheduler->>Scheduler: 检查token预算
        Scheduler->>Scheduler: 选择req1, req2, req3
        
        Note over Scheduler: 构建批处理
        Scheduler->>Batch: 创建批处理
        Note right of Batch: Batch包含:<br/>- req1: 10 tokens<br/>- req2: 5 tokens<br/>- req3: 8 tokens
        
        Scheduler-->>EngineCore: SchedulerOutput
        
        Note over EngineCore,Model: 执行批处理
        EngineCore->>Model: execute_batch()
        Note right of Model: 批处理前向传播:<br/>- 合并输入<br/>- 并行计算<br/>- 输出logits
        Model-->>EngineCore: BatchOutput
        
        Note over EngineCore: 采样和更新
        EngineCore->>EngineCore: sample_tokens()
        EngineCore->>Scheduler: update_from_output()
        
        Note over Scheduler: 更新请求状态
        Scheduler->>Scheduler: req1: 继续生成
        Scheduler->>Scheduler: req2: 完成
        Scheduler->>Scheduler: req3: 继续生成
        
        Note over Scheduler: 移除完成的请求
        Scheduler->>Scheduler: 从运行队列移除req2
        
        Note over EngineCore: 返回输出
        EngineCore-->>API: req1 output
        EngineCore-->>API: req2 output (完成)
        EngineCore-->>API: req3 output
        
        API-->>Req1: 流式token
        API-->>Req2: 最终响应
        API-->>Req3: 流式token
    end
```

## 5. 调度器内部工作流程

下图展示了调度器如何选择和调度请求：

```mermaid
flowchart TD
    Start([调度开始]) --> CheckWaiting{等待队列<br/>有请求?}
    CheckWaiting -->|否| CheckRunning{运行队列<br/>有请求?}
    CheckWaiting -->|是| SelectWaiting[从等待队列选择请求]
    
    SelectWaiting --> CheckBudget{Token预算<br/>充足?}
    CheckBudget -->|否| CheckRunning
    CheckBudget -->|是| AllocKV[分配KV缓存块]
    
    AllocKV --> CheckKV{KV缓存<br/>充足?}
    CheckKV -->|否| Preempt[抢占低优先级请求]
    CheckKV -->|是| AddRunning[添加到运行队列]
    
    Preempt --> FreeKV[释放KV缓存]
    FreeKV --> AddRunning
    
    AddRunning --> CheckRunning
    CheckRunning -->|否| EmptyBatch[返回空批处理]
    CheckRunning -->|是| SelectRunning[从运行队列选择请求]
    
    SelectRunning --> AllocTokens[分配Token预算]
    AllocTokens --> CheckTokens{预算<br/>充足?}
    CheckTokens -->|否| BuildOutput[构建调度输出]
    CheckTokens -->|是| SelectRunning
    
    BuildOutput --> Return([返回SchedulerOutput])
    EmptyBatch --> Return
    
    style Start fill:#e1f5ff
    style Return fill:#c8e6c9
    style Preempt fill:#ffcdd2
    style AllocKV fill:#fff9c4
    style BuildOutput fill:#f3e5f5
```

## 6. KV缓存管理流程

下图展示了KV缓存的分配和管理流程：

```mermaid
sequenceDiagram
    participant Scheduler as 调度器
    participant KVMgr as KV缓存管理器
    participant GPU as GPU内存
    participant CPU as CPU内存
    
    Note over Scheduler: 新请求到达
    Scheduler->>KVMgr: 分配KV缓存块
    
    Note over KVMgr: 检查可用块
    KVMgr->>KVMgr: 检查GPU可用块
    
    alt GPU有可用块
        KVMgr->>GPU: 分配GPU块
        GPU-->>KVMgr: 块ID
        KVMgr-->>Scheduler: GPU块分配成功
    else GPU无可用块
        Note over KVMgr: 检查是否需要卸载
        KVMgr->>KVMgr: 查找可卸载的块
        
        alt 有可卸载块
            KVMgr->>GPU: 卸载块到CPU
            GPU->>CPU: 传输数据
            KVMgr->>GPU: 分配新块
            GPU-->>KVMgr: 块ID
            KVMgr-->>Scheduler: GPU块分配成功
        else 无可用块
            KVMgr->>CPU: 分配CPU块
            CPU-->>KVMgr: 块ID
            KVMgr-->>Scheduler: CPU块分配（延迟）
        end
    end
    
    Note over Scheduler: 请求执行中
    Scheduler->>KVMgr: 更新缓存块
    
    Note over Scheduler: 请求完成
    Scheduler->>KVMgr: 释放KV缓存块
    KVMgr->>GPU: 释放GPU块
    KVMgr->>KVMgr: 标记块为可用
```

## 7. 模型执行详细流程

下图展示了模型执行器、Worker和ModelRunner的详细交互：

```mermaid
sequenceDiagram
    participant EngineCore as EngineCore
    participant Executor as Executor
    participant Worker as Worker
    participant ModelRunner as ModelRunner
    participant Attention as 注意力层
    participant MLP as MLP层
    participant Embed as 嵌入层
    
    EngineCore->>Executor: execute_model(SchedulerOutput)
    
    Note over Executor: 准备批处理
    Executor->>Worker: execute_model()
    Worker->>ModelRunner: execute_model()
    
    Note over ModelRunner: 1. 准备输入
    ModelRunner->>ModelRunner: 构建输入batch
    ModelRunner->>ModelRunner: 准备位置编码
    ModelRunner->>Embed: 嵌入层
    
    Note over Embed: 2. Token嵌入
    Embed->>Embed: token_ids → embeddings
    Embed-->>ModelRunner: embeddings
    
    Note over ModelRunner: 3. Transformer层
    loop 每个Transformer层
        ModelRunner->>Attention: 注意力计算
        Note over Attention: Multi-Head Attention<br/>- Query, Key, Value<br/>- 注意力分数<br/>- 输出投影
        Attention-->>ModelRunner: attention_output
        
        ModelRunner->>MLP: 前馈网络
        MLP-->>ModelRunner: mlp_output
        
        Note over ModelRunner: 残差连接和层归一化
        ModelRunner->>ModelRunner: residual + norm
    end
    
    Note over ModelRunner: 4. 输出层
    ModelRunner->>ModelRunner: 最终层归一化
    ModelRunner->>ModelRunner: 输出投影到vocab_size
    
    ModelRunner-->>Worker: logits [batch_size, seq_len, vocab_size]
    Worker-->>Executor: ModelRunnerOutput
    Executor-->>EngineCore: ModelRunnerOutput
    
    Note over EngineCore: 5. 采样
    EngineCore->>Executor: sample_tokens()
    Executor->>Worker: sample_tokens()
    Worker->>Worker: 应用采样策略<br/>(top-k, top-p, temperature)
    Worker-->>Executor: sampled_token_ids
    Executor-->>EngineCore: sampled_token_ids
```

## 8. 异步处理流程

下图展示了异步请求处理的完整流程：

```mermaid
sequenceDiagram
    participant Client1 as 客户端1
    participant Client2 as 客户端2
    participant API as API服务器
    participant AsyncLLM as AsyncLLM
    participant Queue as 请求队列
    participant EngineCore as EngineCore
    participant Loop as 引擎循环
    
    par 客户端1请求
        Client1->>API: POST /v1/chat/completions
        API->>AsyncLLM: add_request(req1)
        AsyncLLM->>Queue: 添加到队列
        AsyncLLM-->>API: RequestOutputCollector
    and 客户端2请求
        Client2->>API: POST /v1/chat/completions
        API->>AsyncLLM: add_request(req2)
        AsyncLLM->>Queue: 添加到队列
        AsyncLLM-->>API: RequestOutputCollector
    end
    
    Note over Loop: 引擎循环（独立线程/进程）
    loop 引擎循环
        Loop->>EngineCore: step()
        EngineCore->>Queue: 获取待处理请求
        Queue-->>EngineCore: req1, req2
        
        EngineCore->>EngineCore: 调度和执行
        Note over EngineCore: 批处理执行req1和req2
        
        EngineCore->>AsyncLLM: 输出结果
        AsyncLLM->>AsyncLLM: 更新RequestOutputCollector
        
        par 流式返回客户端1
            AsyncLLM-->>Client1: token1
            AsyncLLM-->>Client1: token2
            AsyncLLM-->>Client1: token3
        and 流式返回客户端2
            AsyncLLM-->>Client2: token1
            AsyncLLM-->>Client2: token2
        end
    end
```

## 9. 文件调用关系图

下图展示了主要文件之间的调用关系：

```mermaid
graph TD
    main[main.py] --> serve[serve.py]
    serve --> api_server[api_server.py]
    api_server --> run_server[run_server_worker]
    api_server --> build_app[build_app]
    
    build_app --> serving_chat[serving_chat.py]
    serving_chat --> async_llm[async_llm.py]
    
    async_llm --> input_proc[input_processor.py]
    async_llm --> core[core.py]
    async_llm --> output_proc[output_processor.py]
    
    core --> scheduler[scheduler.py]
    core --> kv_cache[kv_cache_manager.py]
    core --> executor[executor]
    
    scheduler --> request_queue[request_queue.py]
    scheduler --> kv_cache
    
    executor --> uniproc[uniproc_executor.py]
    executor --> multiproc[multiproc_executor.py]
    
    uniproc --> worker[gpu_worker.py]
    multiproc --> worker
    
    worker --> model_runner[gpu_model_runner.py]
    model_runner --> model[model_executor]
    
    style main fill:#e1f5ff
    style api_server fill:#fff4e1
    style core fill:#f3e5f5
    style scheduler fill:#e8f5e9
    style executor fill:#fce4ec
    style worker fill:#e0f2f1
```

## 10. 总结

以上图表展示了vLLM的完整架构和执行流程：

1. **整体架构图**：展示了6个主要层次和组件关系
2. **详细组件架构图**：展示了数据在各个组件间的流动
3. **单请求执行流程**：展示了从HTTP请求到响应的完整序列
4. **多请求批处理流程**：展示了如何批处理多个并发请求
5. **调度器工作流程**：展示了调度器的决策逻辑
6. **KV缓存管理流程**：展示了KV缓存的分配和管理
7. **模型执行详细流程**：展示了模型前向传播的细节
8. **异步处理流程**：展示了异步请求处理机制
9. **文件调用关系图**：展示了主要文件间的调用关系

这些图表可以帮助理解vLLM的整体架构和请求处理流程。
